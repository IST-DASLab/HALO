{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Using PEFT with custom models",
   "id": "602c7c1ca86317f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`peft` allows us to fine-tune models efficiently with LoRA. In this short notebook, we will demonstrate how to train a simple multilayer perceptron (MLP) using `peft`.",
   "id": "9d03570a8ce41e8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "aead81cceb7deed1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Make sure that you have the latest version of `peft` installed. To ensure that, run this in your Python environment:\n",
    "    \n",
    "    python -m pip install --upgrade peft"
   ],
   "id": "5c5883aabc88cf6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import copy\n",
    "import os\n",
    "\n",
    "# ignore bnb warnings\n",
    "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\""
   ],
   "id": "209f332ea4d5f05f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import peft\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ],
   "id": "ddbd2d22b901faf9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.manual_seed(0)",
   "id": "417f8503f2c991de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data",
   "id": "791ffc64b13ad4ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will create a toy dataset consisting of random data for a classification task. There is a little bit of signal in the data, so we should expect that the loss of the model can improve during training.",
   "id": "7a141045bef2c2a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X = torch.rand((1000, 20))\n",
    "y = (X.sum(1) > 10).long()"
   ],
   "id": "81ed9bf24ace9592"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_train = 800\n",
    "batch_size = 64"
   ],
   "id": "ca736a01e5637209"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(X[:n_train], y[:n_train]),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(X[n_train:], y[n_train:]),\n",
    "    batch_size=batch_size,\n",
    ")"
   ],
   "id": "a5c644b8f710019a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "3d56959cb969cbc5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As a model, we use a simple multilayer perceptron (MLP). For demonstration purposes, we use a very large number of hidden units. This is totally overkill for this task but it helps to demonstrate the advantages of `peft`. In more realistic settings, models will also be quite large on average, so this is not far-fetched.",
   "id": "48cf600791a4ebd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_units_hidden=2000):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(20, num_units_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_units_hidden, num_units_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_units_hidden, 2),\n",
    "            nn.LogSoftmax(dim=-1),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.seq(X)"
   ],
   "id": "8b745d5f92c94b72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "26034cd55296d4bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here are just a few training hyper-parameters and a simple function that performs the training and evaluation loop.",
   "id": "a2de0c86acd1c526"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lr = 0.002\n",
    "batch_size = 64\n",
    "max_epochs = 30\n",
    "device = \"cpu\" if not torch.cuda.is_available() else \"cuda\""
   ],
   "id": "b3eb1a5ede6cd391"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train(model, optimizer, criterion, train_dataloader, eval_dataloader, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for xb, yb in train_dataloader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            outputs = model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            train_loss += loss.detach().float()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        for xb, yb in eval_dataloader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            eval_loss += loss.detach().float()\n",
    "\n",
    "        eval_loss_total = (eval_loss / len(eval_dataloader)).item()\n",
    "        train_loss_total = (train_loss / len(train_dataloader)).item()\n",
    "        print(f\"{epoch=:<2}  {train_loss_total=:.4f}  {eval_loss_total=:.4f}\")"
   ],
   "id": "598706831f64b00a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training without peft",
   "id": "583886ca6f27f7fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's start without using `peft` to see what we can expect from the model training.",
   "id": "aefd7e910e2cf3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "module = MLP().to(device)\n",
    "optimizer = torch.optim.Adam(module.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "id": "6afb86a4838e1b4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%time train(module, optimizer, criterion, train_dataloader, eval_dataloader, epochs=max_epochs)",
   "id": "2abe7ab10d5d49fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Okay, so we got an eval loss of ~0.26, which is much better than random.",
   "id": "6c4b6b3771dcf0cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training with peft",
   "id": "7ab946780cf5db2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's train with `peft`. First we check the names of the modules, so that we can configure `peft` to fine-tune the right modules.",
   "id": "c151fb4b8ebfa2b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "[(n, type(m)) for n, m in MLP().named_modules()]",
   "id": "a0e77151f4220951"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next we can define the LoRA config. There is nothing special going on here. We set the LoRA rank to 8 and select the layers `seq.0` and `seq.2` to be used for LoRA fine-tuning. As for `seq.4`, which is the output layer, we set it as `module_to_save`, which means it is also trained but no LoRA is applied.",
   "id": "1e8d6d339e8d5ba1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "*Note: Not all layers types can be fine-tuned with LoRA. At the moment, linear layers, embeddings, `Conv2D` and `transformers.pytorch_utils.Conv1D` are supported.",
   "id": "4a3de56f4aeaa22e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "config = peft.LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"seq.0\", \"seq.2\"],\n",
    "    modules_to_save=[\"seq.4\"],\n",
    ")"
   ],
   "id": "12d3fba259c62715"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now let's create the `peft` model by passing our initial MLP, as well as the config we just defined, to `get_peft_model`.",
   "id": "e909569b3d2ce238"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "module = MLP().to(device)\n",
    "module_copy = copy.deepcopy(module)  # we keep a copy of the original model for later\n",
    "peft_model = peft.get_peft_model(module, config)\n",
    "optimizer = torch.optim.Adam(peft_model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "peft_model.print_trainable_parameters()"
   ],
   "id": "2c32361e347e79e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Checking the numbers, we see that only ~1% of parameters are actually trained, which is what we like to see.\n",
    "\n",
    "Now let's start the training:"
   ],
   "id": "2fb4d03df26a5526"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%time train(peft_model, optimizer, criterion, train_dataloader, eval_dataloader, epochs=max_epochs)",
   "id": "3fd009419645e74e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the end, we see that the eval loss is very similar to the one we saw earlier when we trained without `peft`. This is quite nice to see, given that we are training a much smaller number of parameters.",
   "id": "94df96115484ccf1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Check which parameters were updated",
   "id": "23859b01b86655a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, just to check that LoRA was applied as expected, we check what original weights were updated what weights stayed the same.",
   "id": "8b90ebd7a60e3d6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for name, param in peft_model.base_model.named_parameters():\n",
    "    if \"lora\" not in name:\n",
    "        continue\n",
    "\n",
    "    print(f\"New parameter {name:<13} | {param.numel():>5} parameters | updated\")"
   ],
   "id": "c9ad2455ab2a032"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "params_before = dict(module_copy.named_parameters())\n",
    "for name, param in peft_model.base_model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        continue\n",
    "\n",
    "    name_before = (\n",
    "        name.partition(\".\")[-1].replace(\"original_\", \"\").replace(\"module.\", \"\").replace(\"modules_to_save.default.\", \"\")\n",
    "    )\n",
    "    param_before = params_before[name_before]\n",
    "    if torch.allclose(param, param_before):\n",
    "        print(f\"Parameter {name_before:<13} | {param.numel():>7} parameters | not updated\")\n",
    "    else:\n",
    "        print(f\"Parameter {name_before:<13} | {param.numel():>7} parameters | updated\")"
   ],
   "id": "6325ceea2c4ec5c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "So we can see that apart from the new LoRA weights that were added, only the last layer was updated. Since the LoRA weights and the last layer have comparitively few parameters, this gives us a big boost in efficiency.",
   "id": "33ca3e14ef35c72a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sharing the model through Hugging Face Hub",
   "id": "b7572325354907a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Pushing the model to HF Hub",
   "id": "584c9fdf7d9d71d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With the `peft` model, it is also very easy to push a model the Hugging Face Hub. Below, we demonstrate how it works. It is assumed that you have a valid Hugging Face account and are logged in:",
   "id": "9e99600482d38d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "user = \"BenjaminB\"  # put your user name here\n",
    "model_name = \"peft-lora-with-custom-model\"\n",
    "model_id = f\"{user}/{model_name}\""
   ],
   "id": "e3776584a5dc7886"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "peft_model.push_to_hub(model_id);",
   "id": "ed35fcaa467df4f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As we can see, the adapter size is only 211 kB.",
   "id": "167fe8659017788c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loading the model from HF Hub",
   "id": "1e9c013e8bb3f6c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, it only takes one step to load the model from HF Hub. To do this, we can use `PeftModel.from_pretrained`, passing our base model and the model ID:",
   "id": "83cbf345617cc450"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "loaded = peft.PeftModel.from_pretrained(module_copy, model_id)\n",
    "type(loaded)"
   ],
   "id": "7ee567d6b72110ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's check that the two models produce the same output:",
   "id": "d587fd37fd9080de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_peft = peft_model(X.to(device))\n",
    "y_loaded = loaded(X.to(device))\n",
    "torch.allclose(y_peft, y_loaded)"
   ],
   "id": "f3b19b73f00c2f5b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Clean up",
   "id": "71a4ce91bceded13"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, as a clean up step, you may want to delete the repo.",
   "id": "ce7a21cd4dc0a672"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from huggingface_hub import delete_repo",
   "id": "3bd6205c02d3b81"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "delete_repo(model_id)",
   "id": "bdbc236321c624b2"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
