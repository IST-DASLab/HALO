{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Using VB-LoRA for sequence classification",
   "id": "ec8a15bb77445c25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this example, we fine-tune Roberta on a sequence classification task using VB-LoRA.\n",
    "\n",
    "This notebook is adapted from `examples/sequence_classification/VeRA.ipynb`."
   ],
   "id": "faa5c3f4227e463c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "4ac9a4aa663fda58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    VBLoRAConfig,\n",
    "    PeftType,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm"
   ],
   "id": "123fbb2503b1ae48"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Parameters",
   "id": "d0e6cd8497e5deb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "batch_size = 32\n",
    "model_name_or_path = \"roberta-large\"\n",
    "task = \"mrpc\"\n",
    "peft_type = PeftType.VBLORA\n",
    "device = \"cuda\"\n",
    "num_epochs = 20\n",
    "rank = 4\n",
    "max_length = 128\n",
    "num_vectors = 90\n",
    "vector_length = 256\n",
    "torch.manual_seed(0)"
   ],
   "id": "629c7d0cd4d8aea8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "peft_config = VBLoRAConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    r=rank,\n",
    "    topk=2,\n",
    "    target_modules=['key', 'value', 'query', 'output.dense', 'intermediate.dense'],\n",
    "    num_vectors=num_vectors,\n",
    "    vector_length=vector_length,\n",
    "    save_only_topk_weights=True,\n",
    "    # Set to True to reduce storage space. Note that the saved parameters cannot be used to resume training from checkpoints.\n",
    "    vblora_dropout=0.,\n",
    ")\n",
    "head_lr = 4e-3\n",
    "vector_bank_lr = 1e-3\n",
    "logits_lr = 1e-2"
   ],
   "id": "c3f53842f8914b39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading data",
   "id": "d3b55e446313bd26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if any(k in model_name_or_path for k in (\"gpt\", \"opt\", \"bloom\")):\n",
    "    padding_side = \"left\"\n",
    "else:\n",
    "    padding_side = \"right\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=padding_side)\n",
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ],
   "id": "f7b8f5fec9d4789d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "datasets = load_dataset(\"glue\", task)\n",
    "metric = evaluate.load(\"glue\", task)"
   ],
   "id": "8226f1877c9cd1b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def tokenize_function(examples):\n",
    "    # max_length=None => use the model max length (it's actually the default)\n",
    "    outputs = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, max_length=max_length)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"idx\", \"sentence1\", \"sentence2\"],\n",
    ")\n",
    "\n",
    "# We also rename the 'label' column to 'labels' which is the expected name for labels by the models of the\n",
    "# transformers library\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")"
   ],
   "id": "39039c694e70d961"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def collate_fn(examples):\n",
    "    return tokenizer.pad(examples, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# Instantiate dataloaders.\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, collate_fn=collate_fn, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], shuffle=False, collate_fn=collate_fn, batch_size=batch_size\n",
    ")"
   ],
   "id": "a6bdb3bf33643ea9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preparing the VB-LoRA model",
   "id": "23d4c9be4ca4ccd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, return_dict=True, max_length=None)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "model.print_savable_parameters()"
   ],
   "id": "7564507511643dae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "\n",
    "decay_parameters = get_parameter_names(model, ALL_LAYERNORM_LAYERS)\n",
    "decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "vector_bank_parameters = [name for name, _ in model.named_parameters() if \"vector_bank\" in name]\n",
    "logits_parameters = [name for name, _ in model.named_parameters() if \"logits\" in name]\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if n in decay_parameters and \\\n",
    "                   n not in logits_parameters and n not in vector_bank_parameters],\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"lr\": head_lr,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters and \\\n",
    "                   n not in logits_parameters and n not in vector_bank_parameters],\n",
    "        \"weight_decay\": 0.0,\n",
    "        \"lr\": head_lr,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if n in vector_bank_parameters],\n",
    "        \"lr\": vector_bank_lr,\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if n in logits_parameters],\n",
    "        \"lr\": logits_lr,\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0.06 * (len(train_dataloader) * num_epochs),\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ],
   "id": "c04e453bd1f3740c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "c8e549e1e6e080b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch.to(device)\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        predictions, references = predictions, batch[\"labels\"]\n",
    "        metric.add_batch(\n",
    "            predictions=predictions,\n",
    "            references=references,\n",
    "        )\n",
    "\n",
    "    eval_metric = metric.compute()\n",
    "    print(f\"epoch {epoch}:\", eval_metric)"
   ],
   "id": "14a9cb5c161eafce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Share adapters on the ðŸ¤— Hub",
   "id": "c49beeed9c159bdd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "account_id = ...  # your Hugging Face Hub account ID",
   "id": "950d73d82010abe6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.push_to_hub(f\"{account_id}/roberta-large-peft-vblora\")",
   "id": "a9b7a6f906822e77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load adapters from the Hub\n",
    "\n",
    "You can also directly load adapters from the Hub using the commands below:"
   ],
   "id": "3b2e3eaf14d4b14c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoTokenizer"
   ],
   "id": "987f91f24b1b8cb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "peft_model_id = f\"{account_id}/roberta-large-peft-vblora\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "inference_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)"
   ],
   "id": "63995decc28b0714"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the model\n",
    "inference_model = PeftModel.from_pretrained(inference_model, peft_model_id)"
   ],
   "id": "57b31fe5eebce574"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "inference_model.to(device)\n",
    "inference_model.eval()\n",
    "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "    batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = inference_model(**batch)\n",
    "    predictions = outputs.logits.argmax(dim=-1)\n",
    "    predictions, references = predictions, batch[\"labels\"]\n",
    "    metric.add_batch(\n",
    "        predictions=predictions,\n",
    "        references=references,\n",
    "    )\n",
    "\n",
    "eval_metric = metric.compute()\n",
    "print(eval_metric)"
   ],
   "id": "1c3d9cbd18823e92"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
